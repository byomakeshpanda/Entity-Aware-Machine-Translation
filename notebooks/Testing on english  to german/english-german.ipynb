{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23d408dd-a903-46ed-941d-95cd16bc36c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78bc0eb6-d8be-4e15-ab8f-e8a171761432",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_5000_en_ge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5085e40-c2ab-4434-97a6-5b9a6f9463fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>German</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resumption of the session</td>\n",
       "      <td>Wiederaufnahme der Sitzungsperiode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I declare resumed the session of the European ...</td>\n",
       "      <td>Ich erkläre die am Freitag, dem 17. Dezember u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although, as you will have seen, the dreaded '...</td>\n",
       "      <td>Wie Sie feststellen konnten, ist der gefürchte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You have requested a debate on this subject in...</td>\n",
       "      <td>Im Parlament besteht der Wunsch nach einer Aus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the meantime, I should like to observe a mi...</td>\n",
       "      <td>Heute möchte ich Sie bitten - das ist auch der...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0                          Resumption of the session   \n",
       "1  I declare resumed the session of the European ...   \n",
       "2  Although, as you will have seen, the dreaded '...   \n",
       "3  You have requested a debate on this subject in...   \n",
       "4  In the meantime, I should like to observe a mi...   \n",
       "\n",
       "                                              German  \n",
       "0                 Wiederaufnahme der Sitzungsperiode  \n",
       "1  Ich erkläre die am Freitag, dem 17. Dezember u...  \n",
       "2  Wie Sie feststellen konnten, ist der gefürchte...  \n",
       "3  Im Parlament besteht der Wunsch nach einer Aus...  \n",
       "4  Heute möchte ich Sie bitten - das ist auch der...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2403ee70-6607-4801-b4f6-c00a8f18bdd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "English    17\n",
       "German     11\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49e4b4c2-a691-4a93-8a92-b295da915c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4944b511-4ed4-402d-9ba9-d347241356e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c3c6ec3-e954-4953-88de-70bd42e67459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85521e2a-3ac8-433c-91a4-8fe76b6a875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38e06868-4b79-4c48-961b-36c834927df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41f9c6e5-59df-4871-a8c7-76e5ff34d78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Entities: 100%|█████████████████████████████████████████| 4972/4972 [00:45<00:00, 109.23sentence/s]\n"
     ]
    }
   ],
   "source": [
    "entities_list = []\n",
    "for text in tqdm(df[\"English\"], desc=\"Extracting Entities\", unit=\"sentence\"):\n",
    "    entities_list.append(extract_entities(text))\n",
    "\n",
    "df[\"entities\"] = entities_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ffa41e5-b0f5-4003-bfdc-230752d1561b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>German</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resumption of the session</td>\n",
       "      <td>Wiederaufnahme der Sitzungsperiode</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I declare resumed the session of the European ...</td>\n",
       "      <td>Ich erkläre die am Freitag, dem 17. Dezember u...</td>\n",
       "      <td>[{'entity_name': 'the European Parliament', 'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although, as you will have seen, the dreaded '...</td>\n",
       "      <td>Wie Sie feststellen konnten, ist der gefürchte...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You have requested a debate on this subject in...</td>\n",
       "      <td>Im Parlament besteht der Wunsch nach einer Aus...</td>\n",
       "      <td>[{'entity_name': 'the next few days', 'entity_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the meantime, I should like to observe a mi...</td>\n",
       "      <td>Heute möchte ich Sie bitten - das ist auch der...</td>\n",
       "      <td>[{'entity_name': 'the European Union', 'entity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0                          Resumption of the session   \n",
       "1  I declare resumed the session of the European ...   \n",
       "2  Although, as you will have seen, the dreaded '...   \n",
       "3  You have requested a debate on this subject in...   \n",
       "4  In the meantime, I should like to observe a mi...   \n",
       "\n",
       "                                              German  \\\n",
       "0                 Wiederaufnahme der Sitzungsperiode   \n",
       "1  Ich erkläre die am Freitag, dem 17. Dezember u...   \n",
       "2  Wie Sie feststellen konnten, ist der gefürchte...   \n",
       "3  Im Parlament besteht der Wunsch nach einer Aus...   \n",
       "4  Heute möchte ich Sie bitten - das ist auch der...   \n",
       "\n",
       "                                            entities  \n",
       "0                                               None  \n",
       "1  [{'entity_name': 'the European Parliament', 'e...  \n",
       "2                                               None  \n",
       "3  [{'entity_name': 'the next few days', 'entity_...  \n",
       "4  [{'entity_name': 'the European Union', 'entity...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f98ba71-9d3d-4c92-8811-0d0f89814e0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_name': 'the European Parliament', 'entity_type': 'ORG'},\n",
       " {'entity_name': 'Friday 17 December 1999', 'entity_type': 'DATE'},\n",
       " {'entity_name': 'new year', 'entity_type': 'DATE'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['entities'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e00c4b5-b92b-4201-af93-2a0e2e6eac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"entities\"].notna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c9a45e4-6387-4d0a-a3c5-a231cbd18603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3316, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "828cb11a-af57-426d-bd9a-5d79c16160ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd6cf719-d0b5-4eca-ad64-9124cc73ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    processed_data = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Data\", unit=\"row\"):\n",
    "        source_text = row[\"English\"]\n",
    "        target_text = row[\"German\"]\n",
    "        entities = eval(row[\"entities\"]) if isinstance(row[\"entities\"], str) else row[\"entities\"]\n",
    "\n",
    "        # Create entity annotation text\n",
    "        entity_annotations = [f\"{ent['entity_name']} [{ent['entity_type']}]\" for ent in entities]\n",
    "        entity_text = \", \".join(entity_annotations) if entity_annotations else \"None\"\n",
    "\n",
    "        # Create NER example (1/3rd of the time to balance)\n",
    "        if len(processed_data) % 3 == 0:\n",
    "            processed_data.append({\n",
    "                \"task\": \"NER\",\n",
    "                \"input\": f\"Recognize entities: {source_text}\",\n",
    "                \"output\": entity_text\n",
    "            })\n",
    "\n",
    "        # Create Entity-aware MT example\n",
    "        processed_data.append({\n",
    "            \"task\": \"Entity-aware MT\",\n",
    "            \"input\": f\"Entity translate (EN→DE): {source_text}\",\n",
    "            \"output\": target_text\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f58c8cc-5504-4a98-9927-de5ab0b90294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data: 100%|████████████████████████████████████████████████| 2652/2652 [00:00<00:00, 16974.04row/s]\n",
      "Processing Data: 100%|██████████████████████████████████████████████████| 664/664 [00:00<00:00, 14388.02row/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = preprocess_data(train_df)\n",
    "test_data = preprocess_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a42ba04-ad87-450e-854f-58e5133aa612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(samples):\n",
    "    inputs = tokenizer(samples[\"input\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    targets = tokenizer(samples[\"output\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ea7c99a9-c788-4b7b-a7e6-ea60e3471508",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.apply(tokenize_function, axis=1)\n",
    "test_data = test_data.apply(tokenize_function, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0064f6fa-83c3-4f3f-8b70-a2a1f03314ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(value) for key, value in self.data.iloc[idx].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95e47478-c1e4-43cb-858d-b48577579406",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_data)\n",
    "test_dataset = CustomDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef3200bb-2135-434d-a914-bdd3a147056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "72862141-b65f-43f1-b869-b0a24ac52e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f4e33200-7014-4bd6-9cc8-db3d19127aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"t5_finetuned_de\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e855c2e3-ad15-4e80-a2bb-79d22c8cb99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"Custom loss function to prioritize translation over NER.\"\"\"\n",
    "        labels = inputs.pop(\"labels\")  # Extract target labels\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits  # Get logits\n",
    "\n",
    "        # Compute CrossEntropy loss\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=-100)\n",
    "\n",
    "        # Assign higher weight to translation (80%) and lower weight to NER (20%)\n",
    "        ner_weight = 0.2\n",
    "        translation_weight = 0.8\n",
    "\n",
    "        # Get task type (default to Translation)\n",
    "        task_type = inputs.get(\"task_type\", [\"Translation\"] * logits.shape[0])\n",
    "\n",
    "        # Convert task type to weight tensor\n",
    "        task_weights = torch.tensor(\n",
    "            [ner_weight if \"NER\" in task else translation_weight for task in task_type],\n",
    "            device=logits.device,\n",
    "            dtype=torch.float,\n",
    "        )\n",
    "\n",
    "        # Scale loss by task weights\n",
    "        weighted_loss = loss * task_weights.mean()\n",
    "\n",
    "        return (weighted_loss, outputs) if return_outputs else weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "68250c4c-0aa7-43de-ad4b-e6a3a1ba4010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CSE IIT BHILAI\\AppData\\Local\\Temp\\ipykernel_10256\\2242509506.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6275c98f-9a4e-4726-9ee3-d89875b9bb4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4980' max='4980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4980/4980 43:36, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.317336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.378800</td>\n",
       "      <td>0.254828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.400400</td>\n",
       "      <td>0.209418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.337700</td>\n",
       "      <td>0.176456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.293400</td>\n",
       "      <td>0.144211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.258000</td>\n",
       "      <td>0.121422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.226500</td>\n",
       "      <td>0.101785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.203300</td>\n",
       "      <td>0.088808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.184400</td>\n",
       "      <td>0.079777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.170200</td>\n",
       "      <td>0.075887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4980, training_loss=0.26171860752335513, metrics={'train_runtime': 2616.9965, 'train_samples_per_second': 15.201, 'train_steps_per_second': 1.903, 'total_flos': 6809911870095360.0, 'train_loss': 0.26171860752335513, 'epoch': 10.0})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "26e7a29d-874a-4fd8-bfb5-e5a663b31136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"t5_finetuned_de\")\n",
    "tokenizer.save_pretrained(\"t5_finetuned_de\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e75f26ec-79e1-4adc-b717-38e750cdecdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\cse iit bhilai\\anaconda3\\envs\\byom\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\cse iit bhilai\\anaconda3\\envs\\byom\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\cse iit bhilai\\anaconda3\\envs\\byom\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\cse iit bhilai\\anaconda3\\envs\\byom\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\cse iit bhilai\\anaconda3\\envs\\byom\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 1.0/1.5 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 6.6 MB/s eta 0:00:00\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bff5cc4d-2bf8-4403-b9d2-bddbb3d7b3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\CSE IIT\n",
      "[nltk_data]     BHILAI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "import torch\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9a2c95ab-a991-4e58-a2cb-80856c44f84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating BLEU Score: 100%|███████████████████████████████████████████████| 664/664 [16:05<00:00,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 **BLEU Score:** 0.1695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.16952580660031294"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Function to calculate BLEU score for the test set with progress tracking\n",
    "def calculate_bleu(model, tokenizer, test_df, num_samples=None):\n",
    "    references = []  # Ground truth German translations\n",
    "    hypotheses = []  # Model-generated translations\n",
    "\n",
    "    num_samples = num_samples if num_samples else len(test_df)\n",
    "\n",
    "    for i in tqdm(range(num_samples), desc=\"Calculating BLEU Score\"):\n",
    "        input_text = test_df.iloc[i][\"English\"]\n",
    "        expected_translation = test_df.iloc[i][\"German\"]\n",
    "        \n",
    "        # Generate translation using model\n",
    "        inputs = tokenizer(f\"Entity translate (EN→DE): {input_text}\", return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            output_tokens = model.generate(**inputs, max_length=128)\n",
    "        \n",
    "        predicted_translation = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "        # Tokenize expected and predicted sentences\n",
    "        reference_tokens = nltk.word_tokenize(expected_translation.lower())\n",
    "        hypothesis_tokens = nltk.word_tokenize(predicted_translation.lower())\n",
    "\n",
    "        references.append([reference_tokens])  # BLEU expects a list of reference lists\n",
    "        hypotheses.append(hypothesis_tokens)\n",
    "\n",
    "    # Compute BLEU score\n",
    "    bleu_score = corpus_bleu(references, hypotheses)\n",
    "    print(f\"\\n🔹 **BLEU Score:** {bleu_score:.4f}\")\n",
    "\n",
    "    return bleu_score\n",
    "\n",
    "# Call function to compute BLEU with tqdm\n",
    "calculate_bleu(model, tokenizer, test_df, num_samples=len(test_df))  # Change sample size as needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af303ff-761a-4e53-b402-2ab92e923df5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
