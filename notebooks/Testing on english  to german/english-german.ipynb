{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23d408dd-a903-46ed-941d-95cd16bc36c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78bc0eb6-d8be-4e15-ab8f-e8a171761432",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_5000_en_ge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5085e40-c2ab-4434-97a6-5b9a6f9463fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>German</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resumption of the session</td>\n",
       "      <td>Wiederaufnahme der Sitzungsperiode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I declare resumed the session of the European ...</td>\n",
       "      <td>Ich erkl√§re die am Freitag, dem 17. Dezember u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although, as you will have seen, the dreaded '...</td>\n",
       "      <td>Wie Sie feststellen konnten, ist der gef√ºrchte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You have requested a debate on this subject in...</td>\n",
       "      <td>Im Parlament besteht der Wunsch nach einer Aus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the meantime, I should like to observe a mi...</td>\n",
       "      <td>Heute m√∂chte ich Sie bitten - das ist auch der...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0                          Resumption of the session   \n",
       "1  I declare resumed the session of the European ...   \n",
       "2  Although, as you will have seen, the dreaded '...   \n",
       "3  You have requested a debate on this subject in...   \n",
       "4  In the meantime, I should like to observe a mi...   \n",
       "\n",
       "                                              German  \n",
       "0                 Wiederaufnahme der Sitzungsperiode  \n",
       "1  Ich erkl√§re die am Freitag, dem 17. Dezember u...  \n",
       "2  Wie Sie feststellen konnten, ist der gef√ºrchte...  \n",
       "3  Im Parlament besteht der Wunsch nach einer Aus...  \n",
       "4  Heute m√∂chte ich Sie bitten - das ist auch der...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2403ee70-6607-4801-b4f6-c00a8f18bdd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "English    17\n",
       "German     11\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49e4b4c2-a691-4a93-8a92-b295da915c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4944b511-4ed4-402d-9ba9-d347241356e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c3c6ec3-e954-4953-88de-70bd42e67459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85521e2a-3ac8-433c-91a4-8fe76b6a875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38e06868-4b79-4c48-961b-36c834927df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41f9c6e5-59df-4871-a8c7-76e5ff34d78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Entities: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4972/4972 [00:45<00:00, 109.23sentence/s]\n"
     ]
    }
   ],
   "source": [
    "entities_list = []\n",
    "for text in tqdm(df[\"English\"], desc=\"Extracting Entities\", unit=\"sentence\"):\n",
    "    entities_list.append(extract_entities(text))\n",
    "\n",
    "df[\"entities\"] = entities_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ffa41e5-b0f5-4003-bfdc-230752d1561b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>German</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resumption of the session</td>\n",
       "      <td>Wiederaufnahme der Sitzungsperiode</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I declare resumed the session of the European ...</td>\n",
       "      <td>Ich erkl√§re die am Freitag, dem 17. Dezember u...</td>\n",
       "      <td>[{'entity_name': 'the European Parliament', 'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although, as you will have seen, the dreaded '...</td>\n",
       "      <td>Wie Sie feststellen konnten, ist der gef√ºrchte...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You have requested a debate on this subject in...</td>\n",
       "      <td>Im Parlament besteht der Wunsch nach einer Aus...</td>\n",
       "      <td>[{'entity_name': 'the next few days', 'entity_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the meantime, I should like to observe a mi...</td>\n",
       "      <td>Heute m√∂chte ich Sie bitten - das ist auch der...</td>\n",
       "      <td>[{'entity_name': 'the European Union', 'entity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0                          Resumption of the session   \n",
       "1  I declare resumed the session of the European ...   \n",
       "2  Although, as you will have seen, the dreaded '...   \n",
       "3  You have requested a debate on this subject in...   \n",
       "4  In the meantime, I should like to observe a mi...   \n",
       "\n",
       "                                              German  \\\n",
       "0                 Wiederaufnahme der Sitzungsperiode   \n",
       "1  Ich erkl√§re die am Freitag, dem 17. Dezember u...   \n",
       "2  Wie Sie feststellen konnten, ist der gef√ºrchte...   \n",
       "3  Im Parlament besteht der Wunsch nach einer Aus...   \n",
       "4  Heute m√∂chte ich Sie bitten - das ist auch der...   \n",
       "\n",
       "                                            entities  \n",
       "0                                               None  \n",
       "1  [{'entity_name': 'the European Parliament', 'e...  \n",
       "2                                               None  \n",
       "3  [{'entity_name': 'the next few days', 'entity_...  \n",
       "4  [{'entity_name': 'the European Union', 'entity...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f98ba71-9d3d-4c92-8811-0d0f89814e0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_name': 'the European Parliament', 'entity_type': 'ORG'},\n",
       " {'entity_name': 'Friday 17 December 1999', 'entity_type': 'DATE'},\n",
       " {'entity_name': 'new year', 'entity_type': 'DATE'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['entities'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e00c4b5-b92b-4201-af93-2a0e2e6eac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"entities\"].notna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c9a45e4-6387-4d0a-a3c5-a231cbd18603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3316, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "828cb11a-af57-426d-bd9a-5d79c16160ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd6cf719-d0b5-4eca-ad64-9124cc73ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    processed_data = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Data\", unit=\"row\"):\n",
    "        source_text = row[\"English\"]\n",
    "        target_text = row[\"German\"]\n",
    "        entities = eval(row[\"entities\"]) if isinstance(row[\"entities\"], str) else row[\"entities\"]\n",
    "\n",
    "        # Create entity annotation text\n",
    "        entity_annotations = [f\"{ent['entity_name']} [{ent['entity_type']}]\" for ent in entities]\n",
    "        entity_text = \", \".join(entity_annotations) if entity_annotations else \"None\"\n",
    "\n",
    "        # Create NER example (1/3rd of the time to balance)\n",
    "        if len(processed_data) % 3 == 0:\n",
    "            processed_data.append({\n",
    "                \"task\": \"NER\",\n",
    "                \"input\": f\"Recognize entities: {source_text}\",\n",
    "                \"output\": entity_text\n",
    "            })\n",
    "\n",
    "        # Create Entity-aware MT example\n",
    "        processed_data.append({\n",
    "            \"task\": \"Entity-aware MT\",\n",
    "            \"input\": f\"Entity translate (EN‚ÜíDE): {source_text}\",\n",
    "            \"output\": target_text\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f58c8cc-5504-4a98-9927-de5ab0b90294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2652/2652 [00:00<00:00, 16974.04row/s]\n",
      "Processing Data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 664/664 [00:00<00:00, 14388.02row/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = preprocess_data(train_df)\n",
    "test_data = preprocess_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a42ba04-ad87-450e-854f-58e5133aa612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(samples):\n",
    "    inputs = tokenizer(samples[\"input\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    targets = tokenizer(samples[\"output\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ea7c99a9-c788-4b7b-a7e6-ea60e3471508",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.apply(tokenize_function, axis=1)\n",
    "test_data = test_data.apply(tokenize_function, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0064f6fa-83c3-4f3f-8b70-a2a1f03314ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(value) for key, value in self.data.iloc[idx].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95e47478-c1e4-43cb-858d-b48577579406",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_data)\n",
    "test_dataset = CustomDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef3200bb-2135-434d-a914-bdd3a147056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "72862141-b65f-43f1-b869-b0a24ac52e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f4e33200-7014-4bd6-9cc8-db3d19127aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"t5_finetuned_de\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e855c2e3-ad15-4e80-a2bb-79d22c8cb99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"Custom loss function to prioritize translation over NER.\"\"\"\n",
    "        labels = inputs.pop(\"labels\")  # Extract target labels\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits  # Get logits\n",
    "\n",
    "        # Compute CrossEntropy loss\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=-100)\n",
    "\n",
    "        # Assign higher weight to translation (80%) and lower weight to NER (20%)\n",
    "        ner_weight = 0.2\n",
    "        translation_weight = 0.8\n",
    "\n",
    "        # Get task type (default to Translation)\n",
    "        task_type = inputs.get(\"task_type\", [\"Translation\"] * logits.shape[0])\n",
    "\n",
    "        # Convert task type to weight tensor\n",
    "        task_weights = torch.tensor(\n",
    "            [ner_weight if \"NER\" in task else translation_weight for task in task_type],\n",
    "            device=logits.device,\n",
    "            dtype=torch.float,\n",
    "        )\n",
    "\n",
    "        # Scale loss by task weights\n",
    "        weighted_loss = loss * task_weights.mean()\n",
    "\n",
    "        return (weighted_loss, outputs) if return_outputs else weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "68250c4c-0aa7-43de-ad4b-e6a3a1ba4010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CSE IIT BHILAI\\AppData\\Local\\Temp\\ipykernel_10256\\2242509506.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6275c98f-9a4e-4726-9ee3-d89875b9bb4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4980' max='4980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4980/4980 43:36, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.317336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.378800</td>\n",
       "      <td>0.254828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.400400</td>\n",
       "      <td>0.209418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.337700</td>\n",
       "      <td>0.176456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.293400</td>\n",
       "      <td>0.144211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.258000</td>\n",
       "      <td>0.121422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.226500</td>\n",
       "      <td>0.101785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.203300</td>\n",
       "      <td>0.088808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.184400</td>\n",
       "      <td>0.079777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.170200</td>\n",
       "      <td>0.075887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4980, training_loss=0.26171860752335513, metrics={'train_runtime': 2616.9965, 'train_samples_per_second': 15.201, 'train_steps_per_second': 1.903, 'total_flos': 6809911870095360.0, 'train_loss': 0.26171860752335513, 'epoch': 10.0})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "26e7a29d-874a-4fd8-bfb5-e5a663b31136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"t5_finetuned_de\")\n",
    "tokenizer.save_pretrained(\"t5_finetuned_de\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e75f26ec-79e1-4adc-b717-38e750cdecdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\cse iit bhilai\\anaconda3\\envs\\byom\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\cse iit bhilai\\anaconda3\\envs\\byom\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\cse iit bhilai\\anaconda3\\envs\\byom\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\cse iit bhilai\\anaconda3\\envs\\byom\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\cse iit bhilai\\anaconda3\\envs\\byom\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 1.0/1.5 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 6.6 MB/s eta 0:00:00\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bff5cc4d-2bf8-4403-b9d2-bddbb3d7b3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\CSE IIT\n",
      "[nltk_data]     BHILAI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "import torch\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9a2c95ab-a991-4e58-a2cb-80856c44f84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating BLEU Score: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 664/664 [16:05<00:00,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ **BLEU Score:** 0.1695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.16952580660031294"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Function to calculate BLEU score for the test set with progress tracking\n",
    "def calculate_bleu(model, tokenizer, test_df, num_samples=None):\n",
    "    references = []  # Ground truth German translations\n",
    "    hypotheses = []  # Model-generated translations\n",
    "\n",
    "    num_samples = num_samples if num_samples else len(test_df)\n",
    "\n",
    "    for i in tqdm(range(num_samples), desc=\"Calculating BLEU Score\"):\n",
    "        input_text = test_df.iloc[i][\"English\"]\n",
    "        expected_translation = test_df.iloc[i][\"German\"]\n",
    "        \n",
    "        # Generate translation using model\n",
    "        inputs = tokenizer(f\"Entity translate (EN‚ÜíDE): {input_text}\", return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            output_tokens = model.generate(**inputs, max_length=128)\n",
    "        \n",
    "        predicted_translation = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "        # Tokenize expected and predicted sentences\n",
    "        reference_tokens = nltk.word_tokenize(expected_translation.lower())\n",
    "        hypothesis_tokens = nltk.word_tokenize(predicted_translation.lower())\n",
    "\n",
    "        references.append([reference_tokens])  # BLEU expects a list of reference lists\n",
    "        hypotheses.append(hypothesis_tokens)\n",
    "\n",
    "    # Compute BLEU score\n",
    "    bleu_score = corpus_bleu(references, hypotheses)\n",
    "    print(f\"\\nüîπ **BLEU Score:** {bleu_score:.4f}\")\n",
    "\n",
    "    return bleu_score\n",
    "\n",
    "# Call function to compute BLEU with tqdm\n",
    "calculate_bleu(model, tokenizer, test_df, num_samples=len(test_df))  # Change sample size as needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af303ff-761a-4e53-b402-2ab92e923df5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
